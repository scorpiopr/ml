{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 逻辑回归\n",
    "构建逻辑回归模型来预测学生能否被大学录取\n",
    "## 样本数据可视化\n",
    "训练样本存储在ex2data1.txt文件中，包括申请人两门测试的分数和录用结果，画出样本的散点图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 假设 data 是之前 read_csv 读取的 DataFrame\n",
    "# names=['Exam1', 'Exam2', 'Admitted']\n",
    "def plotData(data):\n",
    "    # 1. 筛选数据：将样本分为 正例(1) 和 负例(0)\n",
    "    positive = data[data['Admitted'] == 1]\n",
    "    negative = data[data['Admitted'] == 0]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # 2. 绘制散点图\n",
    "    # s 为点的大小，c 为颜色，marker 为形状\n",
    "    plt.scatter(positive['Exam1'], positive['Exam2'], \n",
    "                s=50, c='b', marker='+', label='Admitted')\n",
    "    \n",
    "    plt.scatter(negative['Exam1'], negative['Exam2'], \n",
    "                s=50, c='r', marker='o', label='Not Admitted')\n",
    "\n",
    "    # 3. 装饰图表\n",
    "    plt.xlabel('Exam 1 Score')\n",
    "    plt.ylabel('Exam 2 Score')\n",
    "    plt.legend() # 显示图例，区分 Admitted/Not Admitted\n",
    "    plt.title('Scatter Plot of Training Data')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"ex2data1.txt\"\n",
    "    data = pd.read_csv(path, header=None, names=['Exam1', 'Exam2', 'Admitted'])\n",
    "    print(f\"样本数据：\\n{data.head()}\")\n",
    "    plotData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sigmoid函数\n",
    "逻辑回归的假设函数表示为\n",
    "$$h_{\\theta}(x)=g(\\theta^Tx)$$\n",
    "$$g(z)=\\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 Sigmoid 函数\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 生成自变量数据：从 -10 到 10，生成 100 个点\n",
    "    z = np.linspace(-10, 10, 100)\n",
    "    g = sigmoid(z)\n",
    "\n",
    "    # 绘图\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(z, g, 'b-', linewidth=2, label=r'$g(z) = \\frac{1}{1 + e^{-z}}$')\n",
    "\n",
    "    # 装饰图表\n",
    "    plt.axhline(y=0.5, color='r', linestyle='--', linewidth=1, label='Threshold (0.5)') # 画出 0.5 决策线\n",
    "    plt.axvline(x=0, color='gray', linestyle='-', alpha=0.3) # 画出 y 轴\n",
    "\n",
    "    plt.title(\"Sigmoid Function\", fontsize=14)\n",
    "    plt.xlabel(\"z\")\n",
    "    plt.ylabel(\"g(z)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代价函数和梯度下降\n",
    "使用梯度下降算法求解使代价函数最小时的$\\theta$值。逻辑回归的代价函数表示为\n",
    "$$J(\\theta)=\\frac{1}{m}\\sum_{i=1}^m[-y^{(i)}\\log (h_{\\theta}(x^{(i)}))-(1-y^{(i)})\\log (1-h_{\\theta}(x^{(i)}))]$$\n",
    "代价函数偏导数表示为\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j}=\\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x_j^{(i)}\\quad(j=0,1,...,n)$$\n",
    "梯度下降算法：\n",
    "$$\\begin{align*}\n",
    "Repeat:\\{\\theta_j&=\\theta_j-\\alpha\\frac{\\partial}{\\partial \\theta_j}J(\\theta)\\\\\n",
    "&=\\theta_j-\\frac{\\alpha}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x_j^{(i)}\\quad(j=0,1,...,n)\\\\\n",
    "&(同时更新所有\\theta)\\}\n",
    "\\end{align*}$$\n",
    "其中$m$为样本数量，$n$为特征数量。需要注意的是虽然逻辑回归的代价函数偏导数形式和线性回归相同，但$h_{\\theta}(x)$的定义完全不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于 m 条包含 n 个特征的样本（矩阵形状为 (m,n)，归一化的标准流程是按列计算\n",
    "def featureNormalization(X):\n",
    "    mu = np.mean(X, axis=0) # 计算每列的均值\n",
    "    sigma = np.std(X, axis=0) # 计算每列的标准差\n",
    "    sigma[sigma == 0] = 1 # 避免除以 0（针对特征值全部相同的列）\n",
    "    X = (X - mu) / sigma\n",
    "    print(X.head())\n",
    "    return X, \n",
    "\n",
    "def addOnes(X):\n",
    "    ones = np.ones((len(X), 1))\n",
    "    return np.hstack((ones, X))\n",
    "\n",
    "def costFunction(theta, X, y):\n",
    "    theta = theta.reshape(-1, 1) # 确保theta为列向量\n",
    "\n",
    "    h = sigmoid(X @ theta)\n",
    "    eps = 1e-15 # 添加 eps (1e-15) 防止 np.log(0) 导致溢出\n",
    "\n",
    "    term1 = np.log(h + eps) * y\n",
    "    term2 = np.log(1 - h + eps) * (1 - y)\n",
    "    return -np.mean(term1 + term2)\n",
    "    # 另一种矩阵乘法形式的写法：return - 1 / m * (y.T @ np.log(h) + (1 - y.T) @ np.log(1 - h)).item()\n",
    "\n",
    "def grad(X, y, theta):\n",
    "    theta = theta.reshape(-1, 1)\n",
    "\n",
    "    h = sigmoid(X @ theta)\n",
    "    error = h - y\n",
    "    return 1 / len(X) * (X.T @ error)\n",
    "\n",
    "def gradientDescent(X, y, theta, alpha, iters):\n",
    "    cost = np.zeros(iters + 1)\n",
    "\n",
    "    for iter in range(iters):\n",
    "        cost[iter] = costFunction(theta, X, y)\n",
    "        theta -= alpha * grad(X, y, theta)\n",
    "    cost[iter+1] = costFunction(theta, X, y)\n",
    "    return theta, cost\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 读取DataFrame格式的特征和标签数据\n",
    "    cols = data.shape[1]\n",
    "    X = data.iloc[:, :-1]\n",
    "    y = data.iloc[:, cols-1:cols]\n",
    "    # 特征归一化\n",
    "    X = featureNormalization(X)\n",
    "\n",
    "    # 特征和标签转换为ndarray类型的二维矩阵\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "    # X添加theta_0对应列\n",
    "    X = addOnes(X)\n",
    "    print(f\"X:shape{X.shape}\\n{X[:5, :]}\")\n",
    "    print(f\"y:shape{y.shape}\\n{y[:5, :]}\")\n",
    "\n",
    "    # 参数初始化\n",
    "    initial_theta = np.zeros((X.shape[1], 1), dtype=np.float32)\n",
    "    print(f\"theta:shape{initial_theta.shape}\\n{initial_theta}\")\n",
    "\n",
    "    # 计算初始代价函数值，验证代价函数计算准确性\n",
    "    print(f\"initial cost:{costFunction(initial_theta, X, y):.3f}\") # initial cost:0.693\n",
    "\n",
    "    alphas = np.array([0.01, 0.03, 0.1, 0.3])\n",
    "    iters = 1000\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "\n",
    "    for alpha in alphas:\n",
    "        current_theta = initial_theta.copy() # 每次循环都使用初始 theta，防止上一个 alpha 的结果干扰\n",
    "        theta, cost = gradientDescent(X, y, current_theta, alpha, iters)\n",
    "        print(f\"theta for alpha = {alpha}:{theta}\")\n",
    "        print(f\"final cost for alpha = {alpha}:{cost[-1]:.3f}\")\n",
    "        plt.plot(range(len(cost)), cost, linewidth=2, label=f\"alpha={alpha}\")\n",
    "    \n",
    "    plt.title(\"Convergence of Cost Function for different Alphas\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Cost J\")\n",
    "    plt.legend() # 显示不同 alpha 的标签\n",
    "    plt.grid(True)\n",
    "    plt.show() # 最后统一显示\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用库函数计算$\\theta$的值\n",
    "使用scipy.optimize.fmin_tnc计算$\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "\n",
    "#  fmin_tnc 要求梯度返回必须是一维数组\n",
    "def gradOneDimension(theta, X, y): # 注意：fmin_tnc使用的cost和grad函数入参第一个必须是theta，其他入参顺序和args相同\n",
    "    theta = theta.reshape(-1, 1)\n",
    "\n",
    "    h = sigmoid(X @ theta)\n",
    "    error = h - y\n",
    "    return (1 / len(X) * (X.T @ error)).flatten()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    initial_theta_1 = np.zeros(X.shape[1])\n",
    "    print(f\"initial_theta_1:shape{initial_theta_1.shape}\")\n",
    "    result = opt.fmin_tnc(func=costFunction, x0=initial_theta_1, fprime=gradOneDimension, args=(X, y)) # func:待最小化的目标函数；x0:参数的初始值，必须为一维数组；fprime:目标函数的梯度函数；args:传递给目标函数和梯度函数的额外参数\n",
    "    print(f\"result returned by fmin_tnc:\\n{result}\")\n",
    "    print(f\"final cost for theta from fmin_tnc:{costFunction(result[0], X, y):.3f}\") # final cost for theta from fmin_tnc:0.203"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 画决策边界\n",
    "决策边界表示为$\\theta^Tx=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDecisionBoundary(data, theta):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # 1. 筛选数据：将样本分为 正例(1) 和 负例(0)\n",
    "    positive = data[data['Admitted'] == 1]\n",
    "    negative = data[data['Admitted'] == 0]\n",
    "\n",
    "    # 2. 绘制散点图\n",
    "    # s 为点的大小，c 为颜色，marker 为形状\n",
    "    plt.scatter(positive['Exam1'], positive['Exam2'], \n",
    "                s=50, c='b', marker='+', label='Admitted')\n",
    "    \n",
    "    plt.scatter(negative['Exam1'], negative['Exam2'], \n",
    "                s=50, c='r', marker='o', label='Not Admitted')\n",
    "\n",
    "    # 3. 画决策边界\n",
    "    exam1score_uniform_raw = np.linspace(data['Exam1'].min(), data['Exam1'].max(), 100)\n",
    "    exam1score_uniform_scaled = featureNormalization(exam1score_uniform_raw)\n",
    "    exam2score_uniform_scaled = (- theta[0] - theta[1] * exam1score_uniform_scaled) / theta[2]\n",
    "    exam2score_uniform_raw = exam2score_uniform_scaled * exam1score_uniform_raw.std() + exam1score_uniform_raw.mean()\n",
    "    plt.plot(exam1score_uniform_raw, exam2score_uniform_raw, 'g-', label='Decision Boundary')\n",
    "\n",
    "    # 4. 装饰图表\n",
    "    plt.xlabel('Exam 1 Score')\n",
    "    plt.ylabel('Exam 2 Score')\n",
    "    plt.legend() # 显示图例，区分 Admitted/Not Admitted\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    plotDecisionBoundary(data, theta.flatten())\n",
    "    plotDecisionBoundary(data, result[0].flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta, X):\n",
    "    probability = sigmoid(X @ theta)\n",
    "    return [1 if p >= 0.5 else 0 for p in probability]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 使用新样本评估\n",
    "    samp = np.array([45, 85]).reshape(1, -1)\n",
    "    samp = addOnes(samp)\n",
    "    samp_scaled = featureNormalization(samp)\n",
    "    print(f\"Predicted Admission Probability:{sigmoid(samp_scaled @ theta).flatten()}\")\n",
    "\n",
    "    # 使用训练集评估模型准确度\n",
    "    pred = predict(theta, X)\n",
    "    correct = [1 if (a == b) else 0 for (a, b) in zip(pred, y)]\n",
    "    accuracy = sum(correct) % len(correct)\n",
    "    print(f\"accuracy:{accuracy}%\") # accuracy:89%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
