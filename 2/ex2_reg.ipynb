{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正则化逻辑回归\n",
    "使用正则化逻辑回归预测芯片质量是否合格。ex2data2.txt文件包含用于训练的样本数据，包括两项测试的结果和是否合格\n",
    "## 样本数据可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotData(data):\n",
    "    # 1. 筛选数据：将样本分为 正例(1) 和 负例(0)\n",
    "    positive = data[data['accepted'] == 1]\n",
    "    negative = data[data['accepted'] == 0]\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    # 2. 绘制散点图\n",
    "    # s 为点的大小，c 为颜色，marker 为形状\n",
    "    plt.scatter(positive['Test1'], positive['Test2'], \n",
    "                s=50, c='black', marker='+', label='accepted')\n",
    "    \n",
    "    plt.scatter(negative['Test1'], negative['Test2'], \n",
    "                s=50, c='y', marker='o', label='rejected')\n",
    "\n",
    "    # 3. 装饰图表\n",
    "    plt.xlabel('Microchip Test 1')\n",
    "    plt.ylabel('Microchip Test 2')\n",
    "    plt.legend() # 显示图例\n",
    "    plt.title('Scatter Plot of Training Data')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = pd.read_csv(\"ex2data2.txt\", names=['Test1', 'Test2', 'accepted'])\n",
    "    plotData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征映射\n",
    "为了更好得拟合数据，假设函数采用六阶非线性方程，需要基于现有特征进行拓展。映射后的特征向量为\n",
    "$$mapFeature(x)=\\begin{bmatrix}\n",
    "1 \\\\\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_1^2 \\\\\n",
    "x_1x_2 \\\\\n",
    "x_2^2 \\\\\n",
    "\\vdots \\\\\n",
    "x_1x_2^5 \\\\\n",
    "x_2^6\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征映射，包括添加theta_0对应列\n",
    "def mapFeature(X, degree):\n",
    "    out = [np.ones((len(X), 1))] # 不直接初始化为列向量，因为ndarray类型只能在循环中使用np.hstack不断扩充数组，NumPy每次都会在内存中重新开辟空间并复制旧数据，比较低效\n",
    "    X1 = X[:, 0].reshape(-1, 1)\n",
    "    X2 = X[:, 1].reshape(-1, 1)\n",
    "    for i in range(1, degree+1):\n",
    "        for j in range(i+1):\n",
    "            new_feat = np.power(X1, i - j) * np.power(X2, j)\n",
    "            out.append(new_feat) # 使用列表存储所有特征列向量，最后用hstack一次将列表中的列向量横向组合成特征矩阵，性能更优\n",
    "    return np.hstack(out)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    cols = data.shape[1]\n",
    "    X = data.iloc[:, :-1].to_numpy()\n",
    "    y= data.iloc[:, cols-1:cols].to_numpy()\n",
    "\n",
    "    degree = 6\n",
    "    X_map = mapFeature(X, degree)\n",
    "    print(f\"Mapped feature:\\n{X_map[:5, :5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代价函数和梯度\n",
    "为了控制拟合程度，正则化逻辑回归的代价函数表示如下，注意不对$\\theta_0$进行正则化\n",
    "$$J(\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}[-y^{(i)}\\log(h_{\\theta}(x^{(i)}))-(1-y^{(i)})\\log(1-h_{\\theta}(x^{(i)}))]+\\frac{\\lambda}{2m}\\sum_{j=1}^{n}\\theta_j^2$$\n",
    "代价函数偏导数表示为\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_0}&=\\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x_j^{(i)}\\quad(j=0)\\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j}&=\\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x_j^{(i)}+\\frac{\\lambda}{m}\\theta_j\\quad(j=1,2,...,n)\n",
    "\\end{align*}$$\n",
    "梯度下降算法：\n",
    "$$\\begin{align*}\n",
    "Repeat:\\{\\theta_0&=\\theta_0-\\frac{\\alpha}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x_0^{(i)}\n",
    "\\\\\\theta_j&=\\theta_j(1-\\frac{\\alpha\\lambda}{m})-\\frac{\\alpha}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x_j^{(i)}\\quad(j=1,...,n)\\\\\n",
    "&(同时更新所有\\theta)\\}\n",
    "\\end{align*}$$\n",
    "其中$m$为样本数量，$n$为特征数量，$\\alpha$为学习率，$\\lambda$为正则化参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 Sigmoid 函数\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def featureNormalization(X):\n",
    "    X = (X - X.mean()) / X.std()\n",
    "    return X \n",
    "\n",
    "def costFunctionReg(theta, X, y, lambda_):\n",
    "    theta = theta.reshape(-1, 1) # 确保theta为列向量\n",
    "\n",
    "    h = sigmoid(X @ theta)\n",
    "    eps = 1e-15 # 添加 eps (1e-15) 防止 np.log(0) 导致溢出\n",
    "\n",
    "    term1 = np.log(h + eps) * y # 逐元素相乘\n",
    "    term2 = np.log(1 - h + eps) * (1 - y)\n",
    "    return -np.mean(term1 + term2) + lambda_ / (2 * len(X)) * np.sum(np.power(theta[:, 1:], 2)) # 注意：不惩罚 theta[0] (偏置项)\n",
    "\n",
    "def grad(X, y, theta):\n",
    "    theta = theta.reshape(-1, 1)\n",
    "\n",
    "    h = sigmoid(X @ theta)\n",
    "    error = h - y\n",
    "    return 1 / len(X) * (X.T @ error)\n",
    "\n",
    "def gradReg(X, y, theta, lambda_):\n",
    "    theta = theta.reshape(-1, 1)\n",
    "\n",
    "    # 1.计算未正则化的梯度\n",
    "    grad = grad(X, y, theta)\n",
    "\n",
    "    # 2.计算正则化部分\n",
    "    # 创建一个 theta 的副本用于正则化计算\n",
    "    reg_theta = np.copy(theta)\n",
    "    # 关键步骤：将 theta_0 对应的位置设为 0，这样它就不会被正则化\n",
    "    reg_theta[0] = 0\n",
    "    \n",
    "    # 计算最终梯度: 基本梯度 + (lambda/m) * 修正后的theta\n",
    "    grad = grad + (lambda_ / len(X)) * reg_theta\n",
    "    \n",
    "    return grad.flatten() # 展平为一维数组，方便优化函数调用\n",
    "\n",
    "\n",
    "def gradientDescent(X, y, theta, alpha, iters, lambda_):\n",
    "    cost = np.zeros(iters + 1)\n",
    "\n",
    "    for iter in range(iters):\n",
    "        cost[iter] = costFunctionReg(theta, X, y, lambda_)\n",
    "        theta -= alpha * gradReg(X, y, theta, lambda_)\n",
    "    cost[iter+1] = costFunctionReg(theta, X, y, lambda_)\n",
    "    return theta, cost\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 特征归一化\n",
    "    X_map[:, 1:] = featureNormalization(X_map[:, 1:])\n",
    "\n",
    "    print(f\"X:shape{X_map.shape}\\n{X_map[:5, :5]}\")\n",
    "    print(f\"y:shape{y.shape}\\n{y[:5, :]}\")\n",
    "\n",
    "    # 参数初始化\n",
    "    initial_theta = np.zeros((X.shape[1], 1), dtype=np.float32)\n",
    "    print(f\"theta:shape{initial_theta.shape}\\n{initial_theta[:5]}\")\n",
    "\n",
    "    # 计算初始代价函数值，验证代价函数计算准确性\n",
    "    lambda_ = 1\n",
    "    print(f\"initial cost:{costFunctionReg(initial_theta, X, y, lambda_):.3f}\") # initial cost:0.693\n",
    "\n",
    "    alphas = np.array([0.01, 0.03, 0.1, 0.3])\n",
    "    iters = 1000\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "\n",
    "    for alpha in alphas:\n",
    "        current_theta = initial_theta.copy() # 每次循环都使用初始 theta，防止上一个 alpha 的结果干扰\n",
    "        theta, cost = gradientDescent(X, y, current_theta, alpha, iters)\n",
    "        print(f\"theta for alpha = {alpha}:{theta}\")\n",
    "        print(f\"final cost for alpha = {alpha}:{cost[-1]:.3f}\")\n",
    "        plt.plot(range(len(cost)), cost, linewidth=2, label=f\"alpha={alpha}\")\n",
    "    \n",
    "    plt.title(\"Convergence of Cost Function for different Alphas\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Cost J\")\n",
    "    plt.legend() # 显示不同 alpha 的标签\n",
    "    plt.grid(True)\n",
    "    plt.show() # 最后统一显示"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
